""" Logistic regression module """

from numpy.ndarray import ndarray

from sequre.attributes import sequre
from sequre.stdlib.learn.neural_net.activations import asigmoid, asigmoid_with_derivative
from sequre.settings import DEBUG
from utils import batch


class LogReg[T]:
    coef_: T
    optimizer: str

    def __init__(self, initial_weights: T, optimizer: str = "bgd"):
        self.coef_ = initial_weights.copy()
        self.optimizer = optimizer
    
    def fit(self, mpc, X: T, y: T, step: float, epochs: int, verbose: bool = True) -> LogReg[T]:
        self.coef_ = LogReg._fit(mpc, X, y, self.coef_, step, epochs, self.optimizer, verbose)
        return self
    
    def predict(self, mpc, X: T, error: float = 0.0) -> T:
        return LogReg._predict(mpc, X, self.coef_, error)
    
    def loss(self, mpc, X: T, y: T) -> T:
        return LogReg._loss(mpc, X, y, self.coef_)

    def randomize_weights(self, mpc, distribution: str = "uniform"):
        self.coef_ = self.coef_.rand(distribution, mpc)
    
    @staticmethod
    def estimate_step(train, test) -> float:
        cov_max = ndarray.max(train.T @ train)
        ref_max = ndarray.max(train.T @ test)
        return max(1 / (1 << 20), 1 / max(cov_max, ref_max))
    
    def _fit(mpc, X: T, y: T, initial_w: T, step: float, epochs: int, optimizer: str, verbose: bool, debug: Static[int] = DEBUG) -> T:
        # Adding bias
        X_tilde = X.pad_with_value(1, 1, 1, mpc)

        # Gradient descent
        if optimizer == "bgd":
            return LogReg._bgd(mpc, X_tilde, y, initial_w, step, epochs, verbose, debug)
        if optimizer == "mbgd":
            return LogReg._mbgd(mpc, X_tilde, y, initial_w, step, epochs, 10, verbose, debug)
        else:
            raise ValueError(f"LogReg: invalid optimizer passed: {optimizer}")
    
    @sequre
    def _bgd(mpc, X_tilde: T, y: T, initial_w: T, step: float, epochs: int, verbose: bool, debug: Static[int] = DEBUG) -> T:
        if debug:
            print(f"CP{mpc.pid}:\tLog. reg. BGD step size:", step)
        
        # Batched gradient descent
        w = initial_w  # n x 1
        for _ in range(epochs):
            if verbose:
                print(f"CP{mpc.pid}:\tLog. reg. BGD epoch {_ + 1}/{epochs}")
            if debug:
                print(f"CP{mpc.pid}:\t\t weigts avg {ndarray.mean(w.reveal(mpc))} | loss: {LogReg._loss(mpc, X_tilde, y, w).reveal(mpc)[0, 0]}")
            
            dot = X_tilde @ w
            alg_sig, d_alg_sig = asigmoid_with_derivative(mpc, dot)
            w += X_tilde.T @ ((y - alg_sig) * d_alg_sig)
        
        return w
    
    @sequre
    def _mbgd(mpc, X_tilde: T, y: T, initial_w: T, step: float, epochs: int, batches: int, verbose: bool, debug: Static[int] = DEBUG) -> T:
        if debug:
            print(f"CP{mpc.pid}:\tLog. reg. BGD step size:", step)
        
        # Compute mini-batches
        X_mini_batches = batch(mpc, X_tilde, batch_count=batches)
        y_mini_batches = batch(mpc, y, batch_count=batches)
        
        # Mini-batched gradient descent
        w = initial_w
        for _ in range(epochs):
            for i in range(batches):
                if verbose:
                    print(f"CP{mpc.pid}:\tLog. reg. MBGD epoch {_ + 1}/{epochs} -- batch {i + 1}/{batches}")
                if debug:
                    print(f"CP{mpc.pid}:\t\t weigts avg {ndarray.mean(w.reveal(mpc))} | loss: {LogReg._loss(mpc, X_mini_batches[i], y_mini_batches[i], w).reveal(mpc)[0, 0]}")
                
                dot = X_mini_batches[i] @ w
                alg_sig, d_alg_sig = asigmoid_with_derivative(mpc, dot)
                w += X_mini_batches[i].T @ ((y_mini_batches[i] - alg_sig) * d_alg_sig)
        
        return w
    
    @sequre
    def _predict(mpc, X: T, w: T, error: float) -> T:
        prediction = asigmoid(mpc, (X.pad_with_value(1, 1, 1, mpc) @ w))
        
        if error != 0.0:
            return prediction + error
        
        return prediction
    
    @sequre
    def _loss(mpc, X: T, y: T, w: T) -> T:
        l = y - asigmoid(mpc, X @ w)
        return l.T @ l
